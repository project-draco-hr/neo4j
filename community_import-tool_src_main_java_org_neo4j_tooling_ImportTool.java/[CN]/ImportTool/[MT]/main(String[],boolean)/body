{
  Args args=Args.parse(incomingArguments);
  if (asksForUsage(args)) {
    printUsage(System.out);
    return;
  }
  FileSystemAbstraction fs=new DefaultFileSystemAbstraction();
  File storeDir;
  Collection<Option<File[]>> nodesFiles, relationshipsFiles;
  boolean enableStacktrace;
  Number processors=null;
  Input input=null;
  int badTolerance;
  Charset inputEncoding;
  boolean skipBadRelationships, skipDuplicateNodes;
  try {
    storeDir=args.interpretOption(Options.STORE_DIR.key(),Converters.<File>mandatory(),Converters.toFile(),Validators.DIRECTORY_IS_WRITABLE,Validators.CONTAINS_NO_EXISTING_DATABASE);
    nodesFiles=INPUT_FILES_EXTRACTOR.apply(args,Options.NODE_DATA.key());
    relationshipsFiles=INPUT_FILES_EXTRACTOR.apply(args,Options.RELATIONSHIP_DATA.key());
    validateInputFiles(nodesFiles,relationshipsFiles);
    enableStacktrace=args.getBoolean(Options.STACKTRACE.key(),Boolean.FALSE,Boolean.TRUE);
    processors=args.getNumber(Options.PROCESSORS.key(),null);
    IdType idType=args.interpretOption(Options.ID_TYPE.key(),withDefault((IdType)Options.ID_TYPE.defaultValue()),TO_ID_TYPE);
    badTolerance=args.getNumber(Options.BAD_TOLERANCE.key(),(Number)Options.BAD_TOLERANCE.defaultValue()).intValue();
    inputEncoding=Charset.forName(args.get(Options.INPUT_ENCODING.key(),defaultCharset().name()));
    skipBadRelationships=args.getBoolean(Options.SKIP_BAD_RELATIONSHIPS.key(),(Boolean)Options.SKIP_BAD_RELATIONSHIPS.defaultValue(),true);
    skipDuplicateNodes=args.getBoolean(Options.SKIP_DUPLICATE_NODES.key(),(Boolean)Options.SKIP_DUPLICATE_NODES.defaultValue(),true);
    input=new CsvInput(nodeData(inputEncoding,nodesFiles),defaultFormatNodeFileHeader(),relationshipData(inputEncoding,relationshipsFiles),defaultFormatRelationshipFileHeader(),idType,csvConfiguration(args,defaultSettingsSuitableForTests),badCollector(badTolerance,collect(skipBadRelationships,skipDuplicateNodes)));
  }
 catch (  IllegalArgumentException e) {
    throw andPrintError("Input error",e,false);
  }
  LifeSupport life=new LifeSupport();
  JobScheduler jobScheduler=life.add(new Neo4jJobScheduler());
  LogService logService=life.add(new StoreLogService(NullLogProvider.getInstance(),fs,storeDir,jobScheduler));
  life.start();
  org.neo4j.unsafe.impl.batchimport.Configuration config=importConfiguration(processors,defaultSettingsSuitableForTests);
  BatchImporter importer=new ParallelBatchImporter(storeDir,config,logService.getInternalLogProvider(),ExecutionMonitors.defaultVisible());
  printInputSummary(storeDir,nodesFiles,relationshipsFiles);
  boolean success=false;
  try {
    importer.doImport(input);
    success=true;
  }
 catch (  Exception e) {
    throw andPrintError("Import error",e,enableStacktrace);
  }
 finally {
    File badFile=new File(storeDir,BAD_FILE_NAME);
    if (badFile.exists()) {
      System.out.println("There were bad entries which were skipped and logged into " + badFile.getAbsolutePath());
    }
    life.shutdown();
    if (!success) {
      try {
        StoreFile.fileOperation(FileOperation.DELETE,fs,storeDir,null,Iterables.<StoreFile,StoreFile>iterable(StoreFile.values()),false,false,StoreFileType.values());
      }
 catch (      IOException e) {
        System.err.println("Unable to delete store files after an aborted import " + e);
        if (enableStacktrace) {
          e.printStackTrace();
        }
      }
    }
  }
}
